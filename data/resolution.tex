% !TeX root = ../thuthesis-example.tex

\begin{resolution}

This thesis tackles the problem of performing efficient inference on large Mixture of Experts (MoE) models, specifically GPT models. MoE models are critical for scaling deep learning models to massive sizes but inference remains challenging. 

The main contributions made by this thesis include:
The thesis designs and implements ExpertFlow, a high-performance MOE inference framework with multi-GPU parallelism. Results show that ExpertFlow achieves up to 1.95x higher throughput and 13\% lower latency than NVIDIA's FasterTransformer. ExpertFlow is open sourced, which is a solid implementation that can make MoE models more accessible.

The thesis shows that the author has solid basic theoretical and professional knowledge and demonstrates the authorâ€™s ability of independent research. The thesis is well structured and organized. During the thesis defense, the author presented his work clearly and answered questions correctly. The defense committee has approved Gabriele Oliaro's thesis defense and nominated him to be awarded the degree of Master of Science.

\end{resolution}
