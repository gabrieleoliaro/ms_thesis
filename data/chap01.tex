% !TeX root = ../thuthesis-example.tex

\chapter{Introduction}
This chapter situates the project within the broader research landscape to enable readers to evaluate its significance and contributions relative to existing literature. We begin by highlighting the importance of Mixture of Experts (MoE) models in deep learning, motivating our efforts to increase the accessibility of this model to machine learning scientists and the wider public. Next, we provide an overview of the challenges associated with MoE inference and introduce existing work in this field. To guide readers through the following chapters, we include an outline of the thesis, serving as a roadmap for navigating the rest of the document.

\section{Major breakthroughs of MoE models}\label{intro1}
Mixture of Experts models have emerged as a crucial component of the most extensive deep learning models. Although first introduced in 1991~\cite{original_moe}, the MoE architecture gained renewed attention in 2017 when Google~\cite{shazeer2017} published a study demonstrating the advantages of incorporating one or more MoE layers into deep neural networks (DNNs) for tasks such as language modeling or machine translation. Since then, the high scalability potential of the model has made it a thriving area of research, and recent studies~\cite{g-shard,tutel,fastermoe,switch_transformer} have shown ways to overcome the challenges that had initially prevented the widespread adoption of MoE models. These challenges include implementation difficulty, training instabilities, and high communication costs due to load imbalances~\cite{switch_transformer}. In recent years, the MoE architecture has facilitated the development of the largest DNN models ever created, such as the Switch Transformer~\cite{switch_transformer}, which boasts 1.6 trillion parameters, and BaGuaLu~\cite{BaGuaLu}, which has the potential to create a 174-trillion-parameter AI model, despite not being trained to convergence.

Current industry trends demonstrate that there is a competition among industry leaders to create larger and larger models, driven by the realization that simply increasing the number of parameters, dataset size, and corresponding amount of computational resources utilized to train a model may be the winning strategy to improve the model's generalization power~\cite{sutton_2019}. At the same time, hardware capabilities are struggling to keep pace with the ever-increasing computational costs, and many researchers predict the end of Moore's Law is imminent~\cite{rotman_2021}. If these trends continue, it is highly likely that MoE layers will become an irreplaceable component of the most extensive DNN models in the next few years, as it may not be feasible to train the latest models in a dense manner by updating every parameter for each input token in each iteration.


\section{Overview of MoE inference}
As the challenges of training MoEs are being addressed, the MoE architecture is becoming more popular, and pre-trained MoE model checkpoints are becoming available to the research community. For example, Google recently open-sourced the pre-trained checkpoint of Switch-C, the largest version of their SwitchTransformer model, and the first trillion-parameters pre-trained language model to be available on HuggingFace~\cite{switch_huggingface}. The next logical step to allow the general public to benefit from the latest MoE models is to build efficient serving systems to be able to serve predictions at low latency and high throughput. So far, the existing works on MoE inference have been limited. DeepSpeed~\cite{deepspeed-moe} and Fairseq~\cite{fairseq} are the two main publicly-available DNN systems supporting MoE inference. More details about these systems will be offered in Section \ref{chpt2-moe-serving}.

Building a high-performance MoE inference system is challenging for a variety of reasons. First of all, inference requests have stringent latency requirements, so unlike in the training phase, we cannot focus solely on maximizing the throughput. Next, batching requests is complicated by the fact that requests are generally small, can have varying sequence lengths, and their arrival times is not known in advance. This, together with the fact that each request will utilize only a small fraction of the available experts makes it difficult to achieve high GPU utilization rates. Finally, MoE models tend to be larger than dense ones. This is to be expected, since one of the main selling points of MoE layers is that they allow researchers to trade computational costs for a larger number of parameters. The large size of the models means that in many cases we will need multiple GPUs to serve a MoE model, but current inference systems are built on the assumption that each model will fit on a single GPU. Building a model-parallel inference system for MoE, however, is challenging for many reasons, chiefly among all the communication overheads. In fact, current communication collectives or inter-GPU communication libraries such as NCCL~\cite{nccl} are not optimized for the communication patterns found in sparsely-activated models.

\section{Key Contributions}
\noindent The key contributions of the ExpertFlow project are the following:
\begin{itemize}
    \item We propose a fully asynchronous distributed system for MoE inference, where every computation is split into fine-grained non-preemptible tasks that are launched automatically by the runtime as soon as their data dependencies become available.
    \item We integrate incremental decoding with speculative inference and dynamic batching to reduce the latency while maintaining the same quality of the output
    \item We design efficient kernels for the fused experts and attention operators to boost the performance by maximizing the level of parallelism. We implement these kernels in CUDA.
\end{itemize}

\section{Thesis overview}
The thesis is organized as follows. Chapter \ref{chapter-2} offers additional background on the MoE architecture, the Transformer and GPT architectures, and existing inference systems for these models. Chapter \ref{chapter-3} offers an high-level overview of ExpertFlow's design and implementation, introducing each of ExpertFlow's components, and explaining their purpose. In addition, the chapter discusses how we built ExpertFlow on top of the FlexFlow asynchronous distributed system, and how we leveraged the asynchronous nature to perform inference more efficiently than a bulk synchronous parallel (BSP) system would be able to support. Chapter \ref{chapter-5} discusses the techniques that we used to be able to support auto-regressive transformer models.  Chapter \ref{chapter-6} describes the custom kernels we built for MoE models. Chapter \ref{chapter-7} describes our evaluation effort, and includes all our performance measurements. Chapter \ref{chapter-8} is dedicated to the conclusions, lesson learned from the project, limitations of ExpertFlow, and ideas for future work. 
%The following chapters each focus on a single component, describing it in depth, and illustrating the journey that lead us to each of the design, implementation, and configuration choices. 