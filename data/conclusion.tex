% !TeX root = ../thuthesis-example.tex

\chapter{Conclusion}\label{chapter-8}

\section{Summary}
In this thesis, we presented \Project, an asynchronous multi-GPU serving system for Mixture of Experts (MoE) models. We began by introducing the MoE model, motivating the need for building a fast inference system, and referencing the existing works in the area. In the following chapters, we described the \Project system in detail, starting from the overall design, the parallelization plan, the runtime, and the underlying asynchronous distributed task-based platform (Legion). Next, we delved into the three components that allow \Project to obtain a competitive performance when serving MoE-based transformer models in autoregressive fashion: dynamic batching, incremental decoding, and speculative inference. Next, we introduced our optimizations at the kernel level, with a particular focus on the fused experts operator, and the multi-head attention operator. Finally, we evaluated \Project's performance and compared it to FasterTransformer, observing a speedup of up to 1.95x in the overall throughput and up to 13\% in the average latency per request.

\section{Limitations and Future Work}
The system described in this thesis is still under active development as part of a research project that is expected to last for several more months to a year. The current implementation is the first prototype, or the Minimum Viable Product (MVP), or a general-purpose asynchronous inference system that will serve as a backbone for research at CMU. One limitation, of the current system, for example, is the memory overhead, which we are currently working on reducing. We thus expect \Project to evolve in several different directions in the future. In particular, we expect to continue optimizing the performance of the MoE kernel and operator, and we will further develop the speculative inference component.
