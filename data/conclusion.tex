% !TeX root = ../thuthesis-example.tex

\chapter{Conclusion}\label{chapter-8}

\section{Summary}
In this thesis, we presented ExpertFlow, an asynchronous multi-GPU serving system for Mixture of Experts (MoE) models. We began by introducing the MoE model, motivating the need for building a fast inference system, and referencing the existing works in the area. In the following chapters, we described the ExpertFlow system in detail, starting from the overall design, the parallelization plan, the runtime, and the underlying asynchronous distributed task-based platform (Legion). Next, we delved into the three components that allow ExpertFlow to obtain a competitive performance when serving MoE-based transformer models in autoregressive fashion: dynamic batching, incremental decoding, and speculative inference. Next, we introduced our optimizations at the kernel level, with a particular focus on the fused experts operator, and the multi-head attention operator. Finally, we evaluated ExpertFlow's performance and compared it to FasterTransformer, observing a speedup of up to 1.95x in the overall throughput and up to 13\% in the average latency per request.

\section{Limitations and Future Work}
The system described in this thesis is still under active development as part of a research project that is expected to last for several more months to a year. The current implementation is the first prototype, or the Minimum Viable Product (MVP), or a general-purpose asynchronous inference system that will serve as a backbone for research at CMU. One limitation, of the current system, for example, is the lack of support for mixed precision, which we are currently working on implementing, in order to reduce the memory overhead. We are also actively working on adding support for offloading-based generative LLM inference. We thus expect ExpertFlow to evolve in several different directions in the future. In particular, we expect to continue optimizing the performance of the MoE kernel and operator, and we will further develop the speculative inference component.
