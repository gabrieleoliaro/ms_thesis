
@misc{sutton_2019,
	title = {The {Bitter} {Lesson}},
	url = {http://www.incompleteideas.net/IncIdeas/BitterLesson.html},
	urldate = {2022-06-16},
	year = 2019,
	author = {Sutton, Richard}
}

@misc{Turing-NLG,
	title = {{Turing-NLG}: A 17-billion-parameter language model by Microsoft},
	url = {https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/},
	urldate = {2022-07-01},
	year = 2020,
	author = {Rosset, Corby}
}

@inproceedings{brown_2020,
 author = {Brown, Tom and 
    Mann, Benjamin and 
    Ryder, Nick and 
    Subbiah, Melanie and 
    Kaplan, Jared D and 
    Dhariwal, Prafulla and 
    Neelakantan, Arvind and 
    Shyam, Pranav and 
    Sastry, Girish and 
    Askell, Amanda and 
    Agarwal, Sandhini and 
    Herbert-Voss, Ariel and 
    Krueger, Gretchen and 
    Henighan, Tom and 
    Child, Rewon and 
    others},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{kaplan_2020,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{megatron-lm,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@article{mt-nlg,
  title={Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model},
  author={Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and others},
  journal={arXiv preprint arXiv:2201.11990},
  year={2022}
}

@misc{shazeer2017,
  doi = {10.48550/ARXIV.1701.06538},
  url = {https://arxiv.org/abs/1701.06538},
  author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{fastermoe,
    author = {He, Jiaao and Zhai, Jidong and Antunes, Tiago and Wang, Haojie and Luo, Fuwen and Shi, Shangfeng and Li, Qin},
    title = {FasterMoE: Modeling and Optimizing Training of Large-Scale Dynamic Pre-Trained Models},
    year = {2022},
    isbn = {9781450392044},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3503221.3508418},
    doi = {10.1145/3503221.3508418},
    booktitle = {Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
    pages = {120–134},
    numpages = {15},
    keywords = {distributed deep learning, performance modeling, parallelism},
    location = {Seoul, Republic of Korea},
    series = {PPoPP '22}
}


@inproceedings {alpa,
    author = {Lianmin Zheng and Zhuohan Li and Hao Zhang and Yonghao Zhuang and Zhifeng Chen and Yanping Huang and Yida Wang and Yuanzhong Xu and Danyang Zhuo and Eric P. Xing and Joseph E. Gonzalez and Ion Stoica},
    title = {Alpa: Automating Inter- and {Intra-Operator} Parallelism for Distributed Deep Learning},
    booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
    year = {2022},
    isbn = {978-1-939133-28-1},
    address = {Carlsbad, CA},
    pages = {559--578},
    url = {https://www.usenix.org/conference/osdi22/presentation/zheng-lianmin},
    publisher = {USENIX Association},
    month = jul,
}

@misc{size-survey,
  doi = {10.48550/ARXIV.2111.14247},
  url = {https://arxiv.org/abs/2111.14247},
  author = {Yu, Fuxun and Wang, Di and Shangguan, Longfei and Zhang, Minjia and Tang, Xulong and Liu, Chenchen and Chen, Xiang},
  keywords = {Machine Learning (cs.LG), Distributed, Parallel, and Cluster Computing (cs.DC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {A Survey of Large-Scale Deep Learning Serving System Optimization: Challenges and Opportunities},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}


@inproceedings{bert,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  editor    = {Jill Burstein and
               Christy Doran and
               Thamar Solorio},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
               and Short Papers)},
  pages     = {4171--4186},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/n19-1423},
  doi       = {10.18653/v1/n19-1423},
  timestamp = {Wed, 16 Mar 2022 23:55:36 +0100},
  biburl    = {https://dblp.org/rec/conf/naacl/DevlinCLT19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}


@article{medical_application,
    author = {Shen, Dinggang and Wu, Guorong and Suk, Heung-Il},
    title = {Deep Learning in Medical Image Analysis},
    journal = {Annual Review of Biomedical Engineering},
    volume = {19},
    number = {1},
    pages = {221-248},
    year = {2017},
    doi = {10.1146/annurev-bioeng-071516-044442},
        note ={PMID: 28301734},
    URL = { 
            https://doi.org/10.1146/annurev-bioeng-071516-044442
    },
    eprint = { 
            https://doi.org/10.1146/annurev-bioeng-071516-044442
    }
}

@inproceedings{gaugan,
    author = {Park, Taesung and Liu, Ming-Yu and Wang, Ting-Chun and Zhu, Jun-Yan},
    title = {GauGAN: Semantic Image Synthesis with Spatially Adaptive Normalization},
    year = {2019},
    isbn = {9781450363150},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3306305.3332370},
    doi = {10.1145/3306305.3332370},
    booktitle = {ACM SIGGRAPH 2019 Real-Time Live!},
    articleno = {2},
    numpages = {1},
    location = {Los Angeles, California},
    series = {SIGGRAPH '19}
}

@INPROCEEDINGS{mt_with_dl,
    author={Singh, Shashi Pal and Kumar, Ajai and Darbari, Hemant and Singh, Lenali and Rastogi, Anshika and Jain, Shikha},  
    booktitle={2017 International Conference on Computer, Communications and Electronics (Comptelix)},
    title={Machine translation using deep learning: An overview},   
    year={2017},
    volume={},
    number={},
    pages={162-167},  doi={10.1109/COMPTELIX.2017.8003957}
}

@article{dl_for_recsys,
  author    = {Maxim Naumov and
               Dheevatsa Mudigere and
               Hao{-}Jun Michael Shi and
               Jianyu Huang and
               Narayanan Sundaraman and
               Jongsoo Park and
               Xiaodong Wang and
               Udit Gupta and
               Carole{-}Jean Wu and
               Alisson G. Azzolini and
               Dmytro Dzhulgakov and
               Andrey Mallevich and
               Ilia Cherniavskii and
               Yinghai Lu and
               Raghuraman Krishnamoorthi and
               others},
  title     = {Deep Learning Recommendation Model for Personalization and Recommendation
               Systems},
  journal   = {CoRR},
  volume    = {abs/1906.00091},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.00091},
  eprinttype = {arXiv},
  eprint    = {1906.00091},
  timestamp = {Wed, 15 Apr 2020 18:01:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-00091.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{dl_for_advertising,
  title={Deep Learning for User Interest and Response Prediction in Online Display Advertising},
  author={Zhabiz Gharibshah and Xingquan Zhu and Arthur Hainline and Michael Conway},
  journal={Data Science and Engineering},
  year={2020},
  volume={5},
  pages={12-26}
}

@ARTICLE{dl_for_gaming,  
    author={Justesen, Niels and Bontrager, Philip and Togelius, Julian and Risi, Sebastian},
    journal={IEEE Transactions on Games},
    title={Deep Learning for Video Game Playing},
    year={2020},
    volume={12},
    number={1},
    pages={1-20},
    doi={10.1109/TG.2019.2896986}
}

@inproceedings {tvm,
    author = {Tianqi Chen and Thierry Moreau and Ziheng Jiang and Lianmin Zheng and Eddie Yan and Haichen Shen and Meghan Cowan and Leyuan Wang and Yuwei Hu and Luis Ceze and Carlos Guestrin and Arvind Krishnamurthy},
    title = {{TVM}: An Automated {End-to-End} Optimizing Compiler for Deep Learning},
    booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
    year = {2018},
    isbn = {978-1-939133-08-3},
    address = {Carlsbad, CA},
    pages = {578--594},
    url = {https://www.usenix.org/conference/osdi18/presentation/chen},
    publisher = {USENIX Association},
    month = oct,
}

@inproceedings{taso,
    author = {Jia, Zhihao and Padon, Oded and Thomas, James and Warszawski, Todd and Zaharia, Matei and Aiken, Alex},
    title = {TASO: Optimizing Deep Learning Computation with Automatic Generation of Graph Substitutions},
    year = {2019},
    isbn = {9781450368735},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3341301.3359630},
    doi = {10.1145/3341301.3359630},
    booktitle = {Proceedings of the 27th ACM Symposium on Operating Systems Principles},
    pages = {47–62},
    numpages = {16},
    keywords = {formal verification, computation graph substitutions, deep neural network, superoptimization},
    location = {Huntsville, Ontario, Canada},
    series = {SOSP '19}
}

@misc{xla,
	title = {{XLA}: Optimizing Compiler for Machine Learning},
	url = {https://www.tensorflow.org/xla},
	urldate = {2022-07-01},
	year = 2017,
	author = {Google XLA Team}
}

@inproceedings {rammer,
    author = {Lingxiao Ma and Zhiqiang Xie and Zhi Yang and Jilong Xue and Youshan Miao and Wei Cui and Wenxiang Hu and Fan Yang and Lintao Zhang and Lidong Zhou},
    title = {Rammer: Enabling Holistic Deep Learning Compiler Optimizations with {rTasks}},
    booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
    year = {2020},
    isbn = {978-1-939133-19-9},
    pages = {881--897},
    url = {https://www.usenix.org/conference/osdi20/presentation/ma},
    publisher = {USENIX Association},
    month = nov,
}

@article{theano,
  author    = {Rami Al{-}Rfou and
               Guillaume Alain and
               Amjad Almahairi and
               Christof Angerm{\"{u}}ller and
               Dzmitry Bahdanau and
               Nicolas Ballas and
               Fr{\'{e}}d{\'{e}}ric Bastien and
               Justin Bayer and
               Anatoly Belikov and
               Alexander Belopolsky and
               Yoshua Bengio and
               Arnaud Bergeron and
               James Bergstra and
               Valentin Bisson and
               Josh Bleecher Snyder and
              others},
  title     = {Theano: {A} Python framework for fast computation of mathematical
               expressions},
  journal   = {CoRR},
  volume    = {abs/1605.02688},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.02688},
  eprinttype = {arXiv},
  eprint    = {1605.02688},
  timestamp = {Thu, 30 Apr 2020 11:17:18 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Al-RfouAAa16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings {pet,
    author = {Haojie Wang and Jidong Zhai and Mingyu Gao and Zixuan Ma and Shizhi Tang and Liyan Zheng and Yuanzhi Li and Kaiyuan Rong and Yuanyong Chen and Zhihao Jia},
    title = {{PET}: Optimizing Tensor Programs with Partially Equivalent Transformations and Automated Corrections},
    booktitle = {15th USENIX Symposium on Operating Systems Design and Implementation (OSDI 21)},
    year = {2021},
    isbn = {978-1-939133-22-9},
    pages = {37--54},
    url = {https://www.usenix.org/conference/osdi21/presentation/wang},
    publisher = {USENIX Association},
    month = jul,
}

@inbook{ansor,
    author = {Zheng, Lianmin and Jia, Chengfan and Sun, Minmin and Wu, Zhao and Yu, Cody Hao and Haj-Ali, Ameer and Wang, Yida and Yang, Jun and Zhuo, Danyang and Sen, Koushik and Gonzalez, Joseph E. and Stoica, Ion},
    title = {Ansor: Generating High-Performance Tensor Programs for Deep Learning},
    year = {2020},
    isbn = {978-1-939133-19-9},
    publisher = {USENIX Association},
    address = {USA},
    booktitle = {Proceedings of the 14th USENIX Conference on Operating Systems Design and Implementation},
    articleno = {49},
    numpages = {17}
}

@ARTICLE{original_moe,  
    author={Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},  
    journal={Neural Computation},   
    title={Adaptive Mixtures of Local Experts},   
    year={1991},  
    volume={3},  
    number={1},  
    pages={79-87},  
    doi={10.1162/neco.1991.3.1.79}
}

@article{g-shard,
  author    = {Dmitry Lepikhin and
               HyoukJoong Lee and
               Yuanzhong Xu and
               Dehao Chen and
               Orhan Firat and
               Yanping Huang and
               Maxim Krikun and
               Noam Shazeer and
               Zhifeng Chen},
  title     = {GShard: Scaling Giant Models with Conditional Computation and Automatic
               Sharding},
  journal   = {CoRR},
  volume    = {abs/2006.16668},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.16668},
  eprinttype = {arXiv},
  eprint    = {2006.16668},
  timestamp = {Thu, 02 Jul 2020 14:42:48 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-16668.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{switch_transformer,
  author  = {William Fedus and Barret Zoph and Noam Shazeer},
  title   = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {120},
  pages   = {1--39},
  url     = {http://jmlr.org/papers/v23/21-0998.html}
}

@inproceedings{BaGuaLu,
    author = {Ma, Zixuan and 
    He, Jiaao and 
    Qiu, Jiezhong and 
    Cao, Huanqi and 
    Wang, Yuanwei and 
    Sun, Zhenbo and 
    Zheng, Liyan and 
    Wang, Haojie and 
    Tang, Shizhi and 
    Zheng, Tianyu and 
    Lin, Junyang and 
    Feng, Guanyu and 
    Huang, Zeqiang and 
    Gao, Jie and 
    Zeng, Aohan and 
    others},
    title = {BaGuaLu: Targeting Brain Scale Pretrained Models with over 37 Million Cores},
    year = {2022},
    isbn = {9781450392044},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3503221.3508417},
    doi = {10.1145/3503221.3508417},
    booktitle = {Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
    pages = {192–204},
    numpages = {13},
    keywords = {artificial intelligence, heterogeneous architecture, supercomputers, mixture of experts},
    location = {Seoul, Republic of Korea},
    series = {PPoPP '22}
}

@article{fastmoe,
  author    = {Jiaao He and
               Jiezhong Qiu and
               Aohan Zeng and
               Zhilin Yang and
               Jidong Zhai and
               Jie Tang},
  title     = {FastMoE: {A} Fast Mixture-of-Expert Training System},
  journal   = {CoRR},
  volume    = {abs/2103.13262},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.13262},
  eprinttype = {arXiv},
  eprint    = {2103.13262},
  timestamp = {Thu, 14 Oct 2021 09:16:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-13262.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{dynamoe,
  doi = {10.48550/ARXIV.2205.01848},
  url = {https://arxiv.org/abs/2205.01848},
  author = {Kossmann, Ferdinand and Jia, Zhihao and Aiken, Alex},
  keywords = {Machine Learning (cs.LG), Distributed, Parallel, and Cluster Computing (cs.DC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Optimizing Mixture of Experts using Dynamic Recompilations},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{tensorflow_transform,
	title = {{TensorFlow} Graph Transform Tool},
	url = {https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md},
	urldate = {2022-07-01},
	year = 2018,
	author = {TensorFlow Team}
}

@inproceedings{layout_transformations,
  title={Optimizing memory efficiency for deep convolutional neural networks on GPUs},
  author={Li, Chao and Yang, Yi and Feng, Min and Chakradhar, Srimat and Zhou, Huiyang},
  booktitle={SC'16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={633--644},
  year={2016},
  organization={IEEE}
}

@inbook{pytorch,
    author = {Paszke, Adam and 
    Gross, Sam and 
    Massa, Francisco and 
    Lerer, Adam and 
    Bradbury, James and 
    Chanan, Gregory and 
    Killeen, Trevor and 
    Lin, Zeming and 
    Gimelshein, Natalia and 
    Antiga, Luca and 
    Desmaison, Alban and 
    K\"{o}pf, Andreas and 
    Yang, Edward and 
    DeVito, Zach and 
    Raison, Martin and others},
    title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
    year = {2019},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
    articleno = {721},
    numpages = {12}
}

@inproceedings{tensorflow,
    author = {Abadi, Mart\'{\i}n and 
    Barham, Paul and 
    Chen, Jianmin and 
    Chen, Zhifeng and 
    Davis, Andy and 
    Dean, Jeffrey and 
    Devin, Matthieu and 
    Ghemawat, Sanjay and 
    Irving, Geoffrey and 
    Isard, Michael and 
    Kudlur, Manjunath and 
    Levenberg, Josh and 
    Monga, Rajat and 
    Moore, Sherry and 
    Murray, Derek G. and 
    others},
    title = {TensorFlow: A System for Large-Scale Machine Learning},
    year = {2016},
    isbn = {9781931971331},
    publisher = {USENIX Association},
    address = {USA},
    booktitle = {Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation},
    pages = {265–283},
    numpages = {19},
    location = {Savannah, GA, USA},
    series = {OSDI'16}
}

@misc{tensorrt,
	title = {NVIDIA TensorRT},
	url = {https://developer.nvidia.com/tensorrt},
	urldate = {2022-07-01},
	year = 2017,
	author = {NVIDIA}
}


@inproceedings{metaflow,
  title={Optimizing DNN Computation with Relaxed Graph Substitutions},
  author={Zhihao Jia and James J. Thomas and Todd Warszawski and Mingyu Gao and Matei A. Zaharia and Alexander Aiken},
  booktitle={MLSys},
  year={2019}
}

@inproceedings {unity,
author = {Colin Unger and Zhihao Jia and Wei Wu and Sina Lin and Mandeep Baines and Carlos Efrain Quintero Narvaez and Vinay Ramakrishnaiah and Nirmal Prajapati and Pat McCormick and Jamaludin Mohd-Yusof and Xi Luo and Dheevatsa Mudigere and Jongsoo Park and Misha Smelyanskiy and Alex Aiken},
title = {Unity: Accelerating {DNN} Training Through Joint Optimization of Algebraic Transformations and Parallelization},
booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
year = {2022},
isbn = {978-1-939133-28-1},
address = {Carlsbad, CA},
pages = {267--284},
url = {https://www.usenix.org/conference/osdi22/presentation/unger},
publisher = {USENIX Association},
month = jul,
}

@inproceedings{PipeDream-2BW,
  title={Memory-efficient pipeline-parallel dnn training},
  author={Narayanan, Deepak and Phanishayee, Amar and Shi, Kaiyu and Chen, Xie and Zaharia, Matei},
  booktitle={International Conference on Machine Learning},
  pages={7937--7947},
  year={2021},
  organization={PMLR}
}

@inproceedings{varuna,
    author = {Athlur, Sanjith and Saran, Nitika and Sivathanu, Muthian and Ramjee, Ramachandran and Kwatra, Nipun},
    title = {Varuna: Scalable, Low-Cost Training of Massive Deep Learning Models},
    year = {2022},
    isbn = {9781450391627},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3492321.3519584},
    doi = {10.1145/3492321.3519584},
    booktitle = {Proceedings of the Seventeenth European Conference on Computer Systems},
    pages = {472–487},
    numpages = {16},
    keywords = {distributed systems, large scale DNN training, systems for machine learning},
    location = {Rennes, France},
    series = {EuroSys '22}
}


@InProceedings{terapipe,
  title = 	 {TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models},
  author =       {Li, Zhuohan and Zhuang, Siyuan and Guo, Shiyuan and Zhuo, Danyang and Zhang, Hao and Song, Dawn and Stoica, Ion},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {6543--6552},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/li21y/li21y.pdf},
  url = 	 {https://proceedings.mlr.press/v139/li21y.html},
}


@inproceedings{piper,
    author = {Tarnawski, Jakub and Narayanan, Deepak and Phanishayee, Amar},
    title = {Piper: Multidimensional Planner for DNN Parallelization},
    booktitle = {NeurIPS 2021},
    year = {2021},
    month = {December},
    url = {https://www.microsoft.com/en-us/research/publication/piper-multidimensional-planner-for-dnn-parallelization/},
}


@article{ios,
  title={Ios: Inter-operator scheduler for cnn acceleration},
  author={Ding, Yaoyao and Zhu, Ligeng and Jia, Zhihao and Pekhimenko, Gennady and Han, Song},
  journal={Proceedings of Machine Learning and Systems},
  volume={3},
  pages={167--180},
  year={2021}
}

@inproceedings{dapple,
    author = {Fan, Shiqing and Rong, Yi and Meng, Chen and Cao, Zongyan and Wang, Siyu and Zheng, Zhen and Wu, Chuan and Long, Guoping and Yang, Jun and Xia, Lixue and Diao, Lansong and Liu, Xiaoyong and Lin, Wei},
    title = {DAPPLE: A Pipelined Data Parallel Approach for Training Large Models},
    year = {2021},
    isbn = {9781450382946},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3437801.3441593},
    doi = {10.1145/3437801.3441593},
    booktitle = {Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
    pages = {431–445},
    numpages = {15},
    keywords = {pipeline parallelism, hybrid parallelism, deep learning, data parallelism},
    location = {Virtual Event, Republic of Korea},
    series = {PPoPP '21}
}

@inproceedings{flexflow,
 author = {Jia, Zhihao and Zaharia, Matei and Aiken, Alex},
 booktitle = {Proceedings of Machine Learning and Systems},
 editor = {A. Talwalkar and V. Smith and M. Zaharia},
 pages = {1--13},
 title = {Beyond Data and Model Parallelism for Deep Neural Networks.},
 url = {https://proceedings.mlsys.org/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf},
 volume = {1},
 year = {2019}
}

@article {tutel,
    author = {Changho Hwang and Wei Cui and Yifan Xiong and Ziyue Yang and Ze Liu and Han Hu and Zilong Wang and Rafael Salas and Jithin Jose and Prabhat Ram and Joe Chau and Peng Cheng and Fan Yang and Mao Yang and Yongqiang Xiong},
    title = {Tutel: Adaptive Mixture-of-Experts at Scale},
    year = {2022},
    month = jun,
    journal = {CoRR},
    volume= {abs/2206.03382},
    url = {https://arxiv.org/pdf/2206.03382.pdf},
}

@misc{onnxruntime,
  title={ONNX Runtime},
  author={ONNX Runtime developers},
  year={2022},
  howpublished={\url{https://onnxruntime.ai/}},
}

@inproceedings{mesh-tensorflow,
  added-at = {2019-01-31T08:39:31.000+0100},
  author = {Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and others},
  biburl = {https://www.bibsonomy.org/bibtex/22618bf7c6a34eaeab8d6f9d4f92f6e7f/loroch},
  booktitle = {Advances in Neural Information Processing Systems},
  interhash = {0d254815d5b765ede6fe71583d99a6a6},
  intrahash = {2618bf7c6a34eaeab8d6f9d4f92f6e7f},
  keywords = {cluster deep_learning distributed google_tpu model_parallelism tensorflow},
  pages = {10435--10444},
  timestamp = {2019-01-31T08:39:31.000+0100},
  title = {Mesh-tensorflow: Deep learning for supercomputers},
  url = {http://papers.nips.cc/paper/8242-mesh-tensorflow-deep-learning-for-supercomputers},
  year = 2018
}

@article{GSPMD,
  author    = {Yuanzhong Xu and
               HyoukJoong Lee and
               Dehao Chen and
               Blake A. Hechtman and
               Yanping Huang and
               Rahul Joshi and
               Maxim Krikun and
               Dmitry Lepikhin and
               Andy Ly and
               Marcello Maggioni and
               Ruoming Pang and
               Noam Shazeer and
               Shibo Wang and
               Tao Wang and
               Yonghui Wu and
               Zhifeng Chen},
  title     = {{GSPMD:} General and Scalable Parallelization for {ML} Computation
               Graphs},
  journal   = {CoRR},
  volume    = {abs/2105.04663},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.04663},
  eprinttype = {arXiv},
  eprint    = {2105.04663},
  timestamp = {Fri, 14 May 2021 12:13:30 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-04663.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{oneflow,
  author    = {Jinhui Yuan and
               Xinqi Li and
               Cheng Cheng and
               Juncheng Liu and
               Ran Guo and
               Shenghang Cai and
               Chi Yao and
               Fei Yang and
               Xiaodong Yi and
               Chuan Wu and
               Haoran Zhang and
               Jie Zhao},
  title     = {OneFlow: Redesign the Distributed Deep Learning Framework from Scratch},
  journal   = {CoRR},
  volume    = {abs/2110.15032},
  year      = {2021},
  url       = {https://arxiv.org/abs/2110.15032},
  eprinttype = {arXiv},
  eprint    = {2110.15032},
  timestamp = {Mon, 27 Jun 2022 07:41:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-15032.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ray,
  title={Ray: A distributed framework for emerging $\{$AI$\}$ applications},
  author={Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I and others},
  booktitle={13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
  pages={561--577},
  year={2018}
}

@inproceedings {ciel,
    author = {Derek G. Murray and Malte Schwarzkopf and Christopher Smowton and Steven Smith and Anil Madhavapeddy and Steven Hand},
    title = {{CIEL}: A Universal Execution Engine for Distributed {Data-Flow} Computing},
    booktitle = {8th USENIX Symposium on Networked Systems Design and Implementation (NSDI 11)},
    year = {2011},
    address = {Boston, MA},
    url = {https://www.usenix.org/conference/nsdi11/ciel-universal-execution-engine-distributed-data-flow-computing},
    publisher = {USENIX Association},
    month = mar,
}

@InProceedings{ dask,
  author    = { {M}atthew {R}ocklin },
  title     = { {D}ask: {P}arallel {C}omputation with {B}locked algorithms and {T}ask {S}cheduling },
  booktitle = { {P}roceedings of the 14th {P}ython in {S}cience {C}onference },
  pages     = { 126 - 132 },
  year      = { 2015 },
  editor    = { {K}athryn {H}uff and {J}ames {B}ergstra },
  doi       = { 10.25080/Majora-7b98e3ed-013 }
}

@inproceedings{rlib,
  title={RLlib: Abstractions for distributed reinforcement learning},
  author={Liang, Eric and Liaw, Richard and Nishihara, Robert and Moritz, Philipp and Fox, Roy and Goldberg, Ken and Gonzalez, Joseph and Jordan, Michael and Stoica, Ion},
  booktitle={International Conference on Machine Learning},
  pages={3053--3062},
  year={2018},
  organization={PMLR}
}

@article{rlib-flow,
  title={RLlib Flow: Distributed Reinforcement Learning is a Dataflow Problem},
  author={Liang, Eric and Wu, Zhanghao and Luo, Michael and Mika, Sven and Gonzalez, Joseph E and Stoica, Ion},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={5506--5517},
  year={2021}
}

@inproceedings{neural-architecture-search,
title	= {Neural Architecture Search with Reinforcement Learning},
author	= {Barret Zoph and Quoc V. Le},
year	= {2017},
URL	= {https://arxiv.org/abs/1611.01578}
}

@misc{moe-multitask-learning,
  doi = {10.48550/ARXIV.2204.07689},
  url = {https://arxiv.org/abs/2204.07689},
  author = {Gupta, Shashank and Mukherjee, Subhabrata and Subudhi, Krishan and Gonzalez, Eduardo and Jose, Damien and Awadallah, Ahmed H. and Gao, Jianfeng},
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Sparsely Activated Mixture-of-Experts are Robust Multi-Task Learners},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@INPROCEEDINGS{jordan-jacobs-1993,
  author={Jordan, M.I. and Jacobs, R.A.},
  booktitle={Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)}, 
  title={Hierarchical mixtures of experts and the EM algorithm}, 
  year={1993},
  volume={2},
  number={},
  pages={1339-1344 vol.2},
  doi={10.1109/IJCNN.1993.716791}}

 @article{het,
   title={HET: Scaling out Huge Embedding Model Training via Cache-enabled Distributed Framework},
   author={Miao, Xupeng and Zhang, Hailin and Shi, Yining and Nie, Xiaonan and Yang, Zhi and Tao, Yangyu and Cui, Bin},
   journal={arXiv preprint arXiv:2112.07221},
   year={2021}
 }
 
 
 @article{horovod,
  Author = {Alexander Sergeev and Mike Del Balso},
  Journal = {arXiv preprint arXiv:1802.05799},
  Title = {Horovod: fast and easy distributed deep learning in {TensorFlow}},
  Year = {2018}
}


@InProceedings{deepspeed-moe,
  title = 	 {{D}eep{S}peed-{M}o{E}: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation {AI} Scale},
  author =       {Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {18332--18346},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/rajbhandari22a/rajbhandari22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/rajbhandari22a.html},
}


@misc{hetumoe,
  doi = {10.48550/ARXIV.2203.14685},
  url = {https://arxiv.org/abs/2203.14685},
  author = {Nie, Xiaonan and Zhao, Pinxue and Miao, Xupeng and Zhao, Tong and Cui, Bin},
  keywords = {Distributed, Parallel, and Cluster Computing (cs.DC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {HetuMoE: An Efficient Trillion-scale Mixture-of-Expert Distributed Training System},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings {retiarii,
author = {Quanlu Zhang and Zhenhua Han and Fan Yang and Yuge Zhang and Zhe Liu and Mao Yang and Lidong Zhou},
title = {Retiarii: A Deep Learning {Exploratory-Training} Framework},
booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
year = {2020},
isbn = {978-1-939133-19-9},
pages = {919--936},
url = {https://www.usenix.org/conference/osdi20/presentation/zhang-quanlu},
publisher = {USENIX Association},
month = nov,
}

@inproceedings{autokeras,
  title={Auto-keras: An efficient neural architecture search system},
  author={Jin, Haifeng and Song, Qingquan and Hu, Xia},
  booktitle={Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={1946--1956},
  year={2019}
}

@misc{nni,
	title = {{Microsoft} Neural Network Intelligence (NNI)},
	url = {https://github.com/microsoft/nni/},
	urldate = {2022-09-09},
	year = 2022,
	author = {Microsoft NNI Team}
}

@article{ray_tune,
  title={Tune: A research platform for distributed model selection and training},
  author={Liaw, Richard and Liang, Eric and Nishihara, Robert and Moritz, Philipp and Gonzalez, Joseph E and Stoica, Ion},
  journal={arXiv preprint arXiv:1807.05118},
  year={2018}
}

@INPROCEEDINGS{parallelnas,  
    author={Qu, Xiaoyang and Wang, Jianzong and Xiao, Jing},  
    booktitle={2020 IEEE 22nd International Conference on High Performance Computing and Communications; IEEE 18th International Conference on Smart City; IEEE 6th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)},
    title={ParallelNAS: A Parallel and Distributed System for Neural Architecture Search},
    year={2020},
    volume={},
    number={},
    pages={247-254},
    doi={10.1109/HPCC-SmartCity-DSS50907.2020.00031}
}


@InProceedings{enas,
  title = 	 {Efficient Neural Architecture Search via Parameters Sharing},
  author =       {Pham, Hieu and Guan, Melody and Zoph, Barret and Le, Quoc and Dean, Jeff},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4095--4104},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/pham18a/pham18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/pham18a.html},
  abstract = 	 {We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. ENAS constructs a large computational graph, where each subgraph represents a neural network architecture, hence forcing all architectures to share their parameters. A controller is trained with policy gradient to search for a subgraph that maximizes the expected reward on a validation set. Meanwhile a model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Sharing parameters among child models allows ENAS to deliver strong empirical performances, whilst using much fewer GPU-hours than existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On Penn Treebank, ENAS discovers a novel architecture that achieves a test perplexity of 56.3, on par with the existing state-of-the-art among all methods without post-training processing. On CIFAR-10, ENAS finds a novel architecture that achieves 2.89% test error, which is on par with the 2.65% test error of NASNet (Zoph et al., 2018).}
}



@InProceedings{tpot,
  title = 	 {TPOT: A Tree-based Pipeline Optimization Tool for Automating Machine Learning},
  author = 	 {Olson, Randal S. and Moore, Jason H.},
  booktitle = 	 {Proceedings of the Workshop on Automatic Machine Learning},
  pages = 	 {66--74},
  year = 	 {2016},
  editor = 	 {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
  volume = 	 {64},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {24 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v64/olson_tpot_2016.pdf},
  url = 	 {https://proceedings.mlr.press/v64/olson_tpot_2016.html},
  abstract = 	 {As data science becomes more mainstream, there will be an ever-growing demand for data science tools that are more accessible, flexible, and scalable. In response to this demand, automated machine learning (autoML) researchers have begun building systems that automate the process of designing and optimizing machine learning pipelines. In this paper we present TPOT, an open source genetic programming-based autoML system that optimizes a series of feature preprocessors and machine learning models with the goal of maximizing classification accuracy on a supervised classification task. We benchmark TPOT on a series of 150 supervised classification tasks and find that it significantly outperforms a basic machine learning analysis in 22 of them, while experiencing minimal degradation in accuracy on 5 of the benchmarks—all without any domain knowledge nor human input. As such, GP-based autoML systems show considerable promise in the autoML domain.}
}

@inproceedings{mt-dnn2,
    title = "The {M}icrosoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding",
    author = "Liu, Xiaodong  and
      Wang, Yu  and
      Ji, Jianshu  and
      Cheng, Hao  and
      Zhu, Xueyun  and
      Awa, Emmanuel  and
      He, Pengcheng  and
      Chen, Weizhu  and
      Poon, Hoifung  and
      Cao, Guihong  and
      Gao, Jianfeng",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-demos.16",
    doi = "10.18653/v1/2020.acl-demos.16",
    pages = "118--126",
    abstract = "We present MT-DNN, an open-source natural language understanding (NLU) toolkit that makes it easy for researchers and developers to train customized deep learning models. Built upon PyTorch and Transformers, MT-DNN is designed to facilitate rapid customization for a broad spectrum of NLU tasks, using a variety of objectives (classification, regression, structured prediction) and text encoders (e.g., RNNs, BERT, RoBERTa, UniLM). A unique feature of MT-DNN is its built-in support for robust and transferable learning using the adversarial multi-task learning paradigm. To enable efficient production deployment, MT-DNN supports multi-task knowledge distillation, which can substantially compress a deep neural model without significant performance drop. We demonstrate the effectiveness of MT-DNN on a wide range of NLU applications across general and biomedical domains. The software and pre-trained models will be publicly available at https://github.com/namisan/mt-dnn.",
}


@inproceedings{mt-dnn1,
    title = "Multi-Task Deep Neural Networks for Natural Language Understanding",
    author = "Liu, Xiaodong  and
      He, Pengcheng  and
      Chen, Weizhu  and
      Gao, Jianfeng",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1441",
    doi = "10.18653/v1/P19-1441",
    pages = "4487--4496",
    abstract = "In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7{\%} (2.2{\%} absolute improvement) as of February 25, 2019 on the latest GLUE test set. We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. Our code and pre-trained models will be made publicly available.",
}

@inproceedings{mmoe,
author = {Ma, Jiaqi and Zhao, Zhe and Yi, Xinyang and Chen, Jilin and Hong, Lichan and Chi, Ed H.},
title = {Modeling Task Relationships in Multi-Task Learning with Multi-Gate Mixture-of-Experts},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3220007},
doi = {10.1145/3219819.3220007},
abstract = {Neural-based multi-task learning has been successfully used in many real-world large-scale applications such as recommendation systems. For example, in movie recommendations, beyond providing users movies which they tend to purchase and watch, the system might also optimize for users liking the movies afterwards. With multi-task learning, we aim to build a single model that learns these multiple goals and tasks simultaneously. However, the prediction quality of commonly used multi-task models is often sensitive to the relationships between tasks. It is therefore important to study the modeling tradeoffs between task-specific objectives and inter-task relationships. In this work, we propose a novel multi-task learning approach, Multi-gate Mixture-of-Experts (MMoE), which explicitly learns to model task relationships from data. We adapt the Mixture-of-Experts (MoE) structure to multi-task learning by sharing the expert submodels across all tasks, while also having a gating network trained to optimize each task. To validate our approach on data with different levels of task relatedness, we first apply it to a synthetic dataset where we control the task relatedness. We show that the proposed approach performs better than baseline methods when the tasks are less related. We also show that the MMoE structure results in an additional trainability benefit, depending on different levels of randomness in the training data and model initialization. Furthermore, we demonstrate the performance improvements by MMoE on real tasks including a binary classification benchmark, and a large-scale content recommendation system at Google.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1930–1939},
numpages = {10},
keywords = {recommendation system, multi-task learning, neural network, mixture of experts},
location = {London, United Kingdom},
series = {KDD '18}
}

@InProceedings{cross-stitch,
author = {Misra, Ishan and Shrivastava, Abhinav and Gupta, Abhinav and Hebert, Martial},
title = {Cross-Stitch Networks for Multi-Task Learning},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@inproceedings{sluice,
  title={Latent multi-task architecture learning},
  author={Ruder, Sebastian and Bingel, Joachim and Augenstein, Isabelle and S{\o}gaard, Anders},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={4822--4829},
  year={2019}
}

@inproceedings{zhao2019,
author = {Zhao, Zhe and Hong, Lichan and Wei, Li and Chen, Jilin and Nath, Aniruddh and Andrews, Shawn and Kumthekar, Aditee and Sathiamoorthy, Maheswaran and Yi, Xinyang and Chi, Ed},
title = {Recommending What Video to Watch next: A Multitask Ranking System},
year = {2019},
isbn = {9781450362436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3298689.3346997},
doi = {10.1145/3298689.3346997},
abstract = {In this paper, we introduce a large scale multi-objective ranking system for recommending what video to watch next on an industrial video sharing platform. The system faces many real-world challenges, including the presence of multiple competing ranking objectives, as well as implicit selection biases in user feedback. To tackle these challenges, we explored a variety of soft-parameter sharing techniques such as Multi-gate Mixture-of-Experts so as to efficiently optimize for multiple ranking objectives. Additionally, we mitigated the selection biases by adopting a Wide &amp; Deep framework. We demonstrated that our proposed techniques can lead to substantial improvements on recommendation quality on one of the world's largest video sharing platforms.},
booktitle = {Proceedings of the 13th ACM Conference on Recommender Systems},
pages = {43–51},
numpages = {9},
keywords = {recommendation and ranking, selection bias, multitask learning},
location = {Copenhagen, Denmark},
series = {RecSys '19}
}

@inproceedings{zero,
author = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
title = {ZeRO: Memory Optimizations toward Training Trillion Parameter Models},
year = {2020},
isbn = {9781728199986},
publisher = {IEEE Press},
abstract = {Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware.We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create Turing-NLG, the world's largest language model at the time (17B parameters) with record breaking accuracy.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {20},
numpages = {16},
location = {Atlanta, Georgia},
series = {SC '20}
}

@inproceedings{deepspeed,
author = {Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
title = {DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3406703},
doi = {10.1145/3394486.3406703},
abstract = {Explore new techniques in Microsoft's open source library called DeepSpeed, which advances large model training by improving scale, speed, cost, and usability, unlocking the ability to train 100-billion-parameter models. DeepSpeed is compatible with PyTorch. One piece of our library, called ZeRO, is a new parallelized optimizer that greatly reduces the resources needed for model and data parallelism while massively increasing the number of parameters that can be trained. Researchers have used these breakthroughs to create Turing Natural Language Generation (Turing-NLG), which at the time of its release was the largest publicly known language model at 17 billion parameters. In addition we will also go over our latest transformer kernel advancements that led the DeepSpeed team to achieve the world fastest BERT pretraining record.The Zero Redundancy Optimizer (ZeRO) is a novel memory optimization technology for large-scale distributed deep learning. ZeRO can train deep learning models with over 100 billion parameters on the current generation of GPU clusters at three to five times the throughput of the current best system. It also presents a clear path to training models with trillions of parameters, demonstrating an unprecedented leap in deep learning system technology.DeepSpeed brings state-of-the-art training techniques, such as ZeRO, optimized kernels, distributed training, mixed precision, and checkpointing, through lightweight APIs compatible with PyTorch. With just a few lines of code changes to your PyTorch model, you can leverage DeepSpeed to address underlying performance challenges and boost the speed and scale of your training.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {3505–3506},
numpages = {2},
keywords = {machine learning, distributed deep learning},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{archai,
    author = {Hu, Hanzhang and Langford, John and Caruana, Rich and Mukherjee, Saurajit and Horvitz, Eric J and Dey, Debadeepta},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
    publisher = {Curran Associates, Inc.},
    title = {Efficient Forward Architecture Search},
    url = {https://proceedings.neurips.cc/paper/2019/file/6c468ec5a41d65815de23ec1d08d7951-Paper.pdf},
    volume = {32},
    year = {2019}
}


@InProceedings{base,
  title = 	 {BASE Layers: Simplifying Training of Large, Sparse Models},
  author =       {Lewis, Mike and Bhosale, Shruti and Dettmers, Tim and Goyal, Naman and Zettlemoyer, Luke},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {6265--6274},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/lewis21a/lewis21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/lewis21a.html},
  abstract = 	 {We introduce a new balanced assignment of experts (BASE) layer for large language models that greatly simplifies existing high capacity sparse layers. Sparse layers can dramatically improve the efficiency of training and inference by routing each token to specialized expert modules that contain only a small fraction of the model parameters. However, it can be difficult to learn balanced routing functions that make full use of the available experts; existing approaches typically use routing heuristics or auxiliary expert-balancing loss functions. In contrast, we formulate token-to-expert allocation as a linear assignment problem, allowing an optimal assignment in which each expert receives an equal number of tokens. This optimal assignment scheme improves efficiency by guaranteeing balanced compute loads, and also simplifies training by not requiring any new hyperparameters or auxiliary losses. Code is publicly released.}
}


@article{mtl_review_ruder,
  title={An overview of multi-task learning in deep neural networks},
  author={Ruder, Sebastian},
  journal={arXiv preprint arXiv:1706.05098},
  year={2017}
}

@article{mtl_review_crawshaw,
  title={Multi-task learning with deep neural networks: A survey},
  author={Crawshaw, Michael},
  journal={arXiv preprint arXiv:2009.09796},
  year={2020}
}

@INPROCEEDINGS{fully-adaptive-feature-sharing,  
    author={Lu, Yongxi and Kumar, Abhishek and Zhai, Shuangfei and Cheng, Yu and Javidi, Tara and Feris, Rogerio},  
    booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   
    title={Fully-Adaptive Feature Sharing in Multi-Task Networks with Applications in Person Attribute Classification},   
    year={2017},  
    volume={},  
    number={},  
    pages={1131-1140},  
    doi={10.1109/CVPR.2017.126}
}

@inproceedings{routing-networks,
title={Routing Networks: Adaptive Selection of Non-Linear Functions for Multi-Task Learning},
author={Clemens Rosenbaum and Tim Klinger and Matthew Riemer},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=ry8dvM-R-},
}

@book{automl_book,
    editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
    publisher = {Springer},
    title = {Automatic Machine Learning: Methods, Systems, Challenges},
    year = {2019}
}

@inproceedings{automl_chapter,
    author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
    title = {Neural Architecture Search},
    pages = {69-86},
    chapter = {3},
    crossref = {automl}
}

@inproceedings{metaqnn,
title={Designing Neural Network Architectures using Reinforcement Learning},
author={Bowen Baker and Otkrist Gupta and Nikhil Naik and Ramesh Raskar},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=S1c2cvqee}
}

@inproceedings{zhong2018,
  title={Practical block-wise neural network architecture generation},
  author={Zhong, Zhao and Yan, Junjie and Wu, Wei and Shao, Jing and Liu, Cheng-Lin},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2423--2432},
  year={2018}
}

@inproceedings{zoph2018learning,
  title={Learning transferable architectures for scalable image recognition},
  author={Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={8697--8710},
  year={2018}
}

@misc{rotman_2021, title={We're not prepared for the end of Moore's law}, url={https://www.technologyreview.com/2020/02/24/905789/were-not-prepared-for-the-end-of-moores-law/}, journal={MIT Technology Review}, publisher={MIT Technology Review}, author={Rotman, David}, year={2021}, month={Apr}} 

@misc{switch_huggingface, title={Switch Transformers C - 2048 experts (1.6T parameters for 3.1 TB)}, url={https://huggingface.co/google/switch-c-2048}, publisher={HuggingFace}}

@inproceedings{fairseq,
  title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
  author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},
  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},
  year = {2019},
}

@misc{nccl,
author = {},
title = {NVIDIA Collective Communication Library (NCCL)},
publisher={NVIDIA}
howpublished = {\url{https://github.com/NVIDIA/nccl}},
month = {},
year = {},
note = {(Accessed on 03/08/2023)}
}

@article{gpt1,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  publisher={OpenAI},
  journal={Preprint},
  year={2018}
}

@inproceedings{transformer,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{fairseq-checkpoint,
    title = "Efficient Large Scale Language Modeling with Mixtures of Experts",
    author = "Artetxe, Mikel  and
      Bhosale, Shruti  and
      Goyal, Naman  and
      Mihaylov, Todor  and
      Ott, Myle  and
      Shleifer, Sam  and
      Lin, Xi Victoria  and
      Du, Jingfei  and
      Iyer, Srinivasan  and
      Pasunuru, Ramakanth  and
      Anantharaman, Giridharan  and
      Li, Xian  and
      Chen, Shuohui  and
      Akin, Halil  and
      Baines, Mandeep  and
      Martin, Louis  and
      Zhou, Xing  and
      Koura, Punit Singh  and
      O{'}Horo, Brian  and
      Wang, Jeffrey  and
      Zettlemoyer, Luke  and
      Diab, Mona  and
      Kozareva, Zornitsa  and
      Stoyanov, Veselin",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.804",
    pages = "11699--11732",
}

@misc{speculative-decoding-google,
      title={Fast Inference from Transformers via Speculative Decoding}, 
      author={Yaniv Leviathan and Matan Kalman and Yossi Matias},
      year={2022},
      eprint={2211.17192},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{speculative-decoding-berkeley,
      title={Big Little Transformer Decoder}, 
      author={Sehoon Kim and Karttikeya Mangalam and Jitendra Malik and Michael W. Mahoney and Amir Gholami and Kurt Keutzer},
      year={2023},
      eprint={2302.07863},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings {orca,
author = {Gyeong-In Yu and Joo Seong Jeong and Geon-Woo Kim and Soojeong Kim and Byung-Gon Chun},
title = {Orca: A Distributed Serving System for {Transformer-Based} Generative Models},
booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
year = {2022},
isbn = {978-1-939133-28-1},
address = {Carlsbad, CA},
pages = {521--538},
url = {https://www.usenix.org/conference/osdi22/presentation/yu},
publisher = {USENIX Association},
month = jul,
}

@misc{faster_transformer,
	title = {FasterTransformer},
	url = {https://github.com/NVIDIA/FasterTransformer},
	urldate = {2023-04-09},
	year = 2023,
	author = {NVIDIA}
}

@inproceedings{turbo_transformers,
  title={TurboTransformers: an efficient GPU serving system for transformer models},
  author={Fang, Jiarui and Yu, Yang and Zhao, Chengduo and Zhou, Jie},
  booktitle={Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={389--402},
  year={2021}
}

@misc{modelscope-checkpoint,
author = {},
title = {GPT-MoE中文67亿诗歌生成模型},
howpublished = {\url{https://www.modelscope.cn/models/PAI/nlp_gpt3_text-generation_0.35B_MoE-64/summary}},
month = {},
year = {},
note = {(Accessed on 04/10/2023)}
}

@inproceedings{swin-transformer,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@article{vit,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@misc{gpt-j,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}

@article{longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@misc{triton-ft-backend,
author = {NVIDIA},
title = {Triton FasterTransformer Backend},
howpublished = {\url{https://github.com/triton-inference-server/fastertransformer_backend}},
month = {},
year = {},
note = {(Accessed on 04/10/2023)}
}

@misc{triton,
author = {NVIDIA},
title = {The Triton Inference Server},
howpublished = {\url{https://github.com/triton-inference-server/server}},
month = {},
year = {},
note = {(Accessed on 04/10/2023)}
}

@article{wav2vec,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12449--12460},
  year={2020}
}

@inproceedings{vivit,
  title={Vivit: A video vision transformer},
  author={Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu{\v{c}}i{\'c}, Mario and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6836--6846},
  year={2021}
}

@INPROCEEDINGS{legion,
  author={Bauer, Michael and Treichler, Sean and Slaughter, Elliott and Aiken, Alex},
  booktitle={SC '12: Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis}, 
  title={Legion: Expressing locality and independence with logical regions}, 
  year={2012},
  volume={},
  number={},
  pages={1-11},
  doi={10.1109/SC.2012.71}}

@misc{chatgpt,
  author = "{OpenAI}",
  title = {ChatGPT},
  howpublished = {\url{https://chat.openai.com/chat}},
  note = {}
}

@misc{vicuna,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://vicuna.lmsys.org},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@incollection{blelloch_prefix_1990,
  added-at = {2009-10-12T12:20:34.000+0200},
  author = {Blelloch, Guy E},
  biburl = {https://www.bibsonomy.org/bibtex/2328d1683c317526dc6f597199430756b/cgray},
  booktitle = {Sythesis of parallel algorithms},
  interhash = {3c5a1d4bbec71389d748dba848448921},
  intrahash = {328d1683c317526dc6f597199430756b},
  keywords = {imported parallel-algorithms},
  pages = {35---60},
  publisher = {Morgan Kaufmann Publishers Inc.},
  timestamp = {2009-10-13T16:37:35.000+0200},
  title = {Prefix Sums and Their Applications},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.47.6430},
  year = 1990
}

@misc{thrust,
author = {NVIDIA},
title = {NVIDIA/thrust: The C++ parallel algorithms library.},
howpublished = {\url{https://github.com/NVIDIA/thrust}},
month = {},
year = {},
note = {(Accessed on 04/24/2023)}
}

@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@inproceedings{fairseq-moe-model,
    title = "Efficient Large Scale Language Modeling with Mixtures of Experts",
    author = "Artetxe, Mikel  and
      Bhosale, Shruti  and
      Goyal, Naman  and
      Mihaylov, Todor  and
      Ott, Myle  and
      Shleifer, Sam  and
      Lin, Xi Victoria  and
      Du, Jingfei  and
      Iyer, Srinivasan  and
      Pasunuru, Ramakanth  and
      Anantharaman, Giridharan  and
      Li, Xian  and
      Chen, Shuohui  and
      Akin, Halil  and
      Baines, Mandeep  and
      Martin, Louis  and
      Zhou, Xing  and
      Koura, Punit Singh  and
      O{'}Horo, Brian  and
      Wang, Jeffrey  and
      Zettlemoyer, Luke  and
      Diab, Mona  and
      Kozareva, Zornitsa  and
      Stoyanov, Veselin",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.804",
    pages = "11699--11732",
    abstract = "Mixture of Experts layers (MoEs) enable efficient scaling of language models through conditional computation. This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models in a wide range of settings: in- and out-of-domain language modeling, zero- and few-shot priming, and full-shot fine-tuning. With the exception of fine-tuning, we find MoEs to be substantially more compute efficient. At more modest training budgets, MoEs can match the performance of dense models using {\textasciitilde}4 times less compute. This gap narrows at scale, but our largest MoE model (1.1T parameters) consistently outperforms a compute-equivalent dense model (6.7B parameters). Overall, this performance gap varies greatly across tasks and domains, suggesting that MoE and dense models generalize differently in ways that are worthy of future study. We make our code and models publicly available for research use.",
}